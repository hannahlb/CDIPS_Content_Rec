{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Read *Wikipedia.csv* as written via Wikipedia class.\n",
    "2. Tokenize the text, and save tokens into *df_token*.\n",
    "3. Build similarity matrix and write this to file *Wikipedia-sims.csv* with columns:  \n",
    "    id : *article id*  \n",
    "    similar_articles : *list of article ids*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data\n",
    "\n",
    "Read .csv files and make a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website</th>\n",
       "      <th>title</th>\n",
       "      <th>page_type</th>\n",
       "      <th>text_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43590845</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Whale_feces</td>\n",
       "      <td>Whale feces</td>\n",
       "      <td>article</td>\n",
       "      <td>[[File:WhalePump.jpg|thumb|400px|right|\"Whale ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42614454</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Whale_watching_i...</td>\n",
       "      <td>Whale watching in New Zealand</td>\n",
       "      <td>article</td>\n",
       "      <td>{{Use dmy dates|date=May 2017}}\\n[[File:Whale ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49735416</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Tail_sailing</td>\n",
       "      <td>Tail sailing</td>\n",
       "      <td>article</td>\n",
       "      <td>[[File:Southern right whale4.jpg|thumb|upright...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52243894</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Bubble_net_feeding</td>\n",
       "      <td>Bubble net feeding</td>\n",
       "      <td>article</td>\n",
       "      <td>{{main|Cetacean surfacing behaviour}}\\n'''Bubb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53720250</th>\n",
       "      <td>https://en.wikipedia.org/wiki/Ethelbert_(whale)</td>\n",
       "      <td>Ethelbert (whale)</td>\n",
       "      <td>article</td>\n",
       "      <td>{{no footnotes|date=June 2017}}\\n\\n'''Ethelber...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    website  \\\n",
       "43590845          https://en.wikipedia.org/wiki/Whale_feces   \n",
       "42614454  https://en.wikipedia.org/wiki/Whale_watching_i...   \n",
       "49735416         https://en.wikipedia.org/wiki/Tail_sailing   \n",
       "52243894   https://en.wikipedia.org/wiki/Bubble_net_feeding   \n",
       "53720250    https://en.wikipedia.org/wiki/Ethelbert_(whale)   \n",
       "\n",
       "                                  title page_type  \\\n",
       "43590845                    Whale feces   article   \n",
       "42614454  Whale watching in New Zealand   article   \n",
       "49735416                   Tail sailing   article   \n",
       "52243894             Bubble net feeding   article   \n",
       "53720250              Ethelbert (whale)   article   \n",
       "\n",
       "                                                   text_raw  \n",
       "43590845  [[File:WhalePump.jpg|thumb|400px|right|\"Whale ...  \n",
       "42614454  {{Use dmy dates|date=May 2017}}\\n[[File:Whale ...  \n",
       "49735416  [[File:Southern right whale4.jpg|thumb|upright...  \n",
       "52243894  {{main|Cetacean surfacing behaviour}}\\n'''Bubb...  \n",
       "53720250  {{no footnotes|date=June 2017}}\\n\\n'''Ethelber...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_dir=os.getcwd()\n",
    "os.chdir('../data')\n",
    "csvfile1 = 'Wikipedia-dog.csv'\n",
    "csvfile2 = 'Wikipedia-fish.csv'\n",
    "\n",
    "df=pd.read_csv(csvfile1,index_col=0)\n",
    "df2=pd.read_csv(csvfile2,index_col=0)\n",
    "df=df.append(df2)\n",
    "del df2\n",
    "\n",
    "df=df[df['page_type']=='article']\n",
    "\n",
    "df.tail(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's use nltk to tokenize and clean up the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tokenizer\n",
    "\n",
    "Find a list of tokens from raw text for each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1467938     [about, shelter, for, dogs, and, cats, for, th...\n",
       "275388      [cynology, ipac, en, s, ᵻ, ˈ, n, ɒ, l, ə, dʒ, ...\n",
       "2352562     [other, uses, wolfpack, disambiguation, image,...\n",
       "17021807    [for, a, list, of, rare, dog, breeds, category...\n",
       "20777185    [refimprove, date, december, 2008, originalres...\n",
       "Name: text_raw, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "df_token=df['text_raw']\n",
    "\n",
    "# Convert to lower case:\n",
    "for index in df_token.index:\n",
    "    text=df_token[index]\n",
    "    df_token[index]=text.lower()\n",
    "\n",
    "# Tokenize\n",
    "df_token=df_token.apply(tokenizer.tokenize)\n",
    "df_token.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim dictionary\n",
    "https://radimrehurek.com/gensim/corpora/dictionary.html\n",
    "\n",
    "* compactify()\n",
    "\n",
    "\n",
    "Assign new word ids to all words.\n",
    "\n",
    "This is done to make the ids more compact, e.g. after some tokens have been removed via filter_tokens() and there are gaps in the id series. Calling this method will remove the gaps.\n",
    "\n",
    "* self.dfs()\n",
    "\n",
    "token frequency\n",
    "\n",
    "* Download stop words running:\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words: ['i', 'me', 'my', 'myself', 'we']\n",
      "\n",
      "Dictionary after filtering:\n",
      "[(1, 'dogs'), (2, 'cats'), (3, 'article'), (4, 'shed')]\n"
     ]
    }
   ],
   "source": [
    "# make gensim dictionary\n",
    "\n",
    "line_list = df_token.values\n",
    "dictionary = corpora.Dictionary(line_list)\n",
    "# dictionary: 0: \"about\", 1:\"shelter\",...\n",
    "\n",
    "# filter dictionary to remove stopwords and words occurring < min_count times\n",
    "# need to run nltk.download() -> 3 GB downloaded into C:\\Users\\melanie\\AppData\\Roaming\\nltk_data\n",
    "stop_words = nltk.corpus.stopwords.words('english') \n",
    "print(\"Stop words: {}\\n\".format(stop_words[:5]))\n",
    "\n",
    "stop_ids = [dictionary.token2id[word] for word in stop_words\n",
    "            if word in dictionary.token2id]\n",
    "min_count = 2\n",
    "rare_ids = [id for id, freq in dictionary.dfs.items()\n",
    "            if freq < min_count]\n",
    "dictionary.filter_tokens(stop_ids + rare_ids)\n",
    "print(\"Dictionary after filtering:\")\n",
    "print([(key,dictionary[key]) for key in dictionary.keys()[1:5]])\n",
    "dictionary.compactify()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2bow()\n",
    "\n",
    "1. counts the number of occurrences of each distinct word\n",
    "2. converts the word to its integer word id \n",
    "3. returns the result as a sparse vector. \n",
    "\n",
    "The sparse vector [(0, 1), (1, 1)] therefore reads: in the document “Human computer interaction”, the words computer (id 0) and human (id 1) appear once; the other ten dictionary words appear (implicitly) zero times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus contains tuples of word lists and its frequency\n",
      "Corpus: [(6, 36), (9, 2), (17, 1), (18, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(words) for words in line_list]\n",
    "print(\"Corpus contains tuples of word lists and its frequency\")\n",
    "print(\"Corpus: {}\".format(corpus[1][1:5]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model transformations in gensim\n",
    "\n",
    "https://radimrehurek.com/gensim/tut2.html#available-transformations\n",
    "\n",
    "1. **Latent Sematic Indexing, LSI (or LSA)** transform documents in a tfldf-weighted space into a latent space of lower dimensionality. On real corpora, target dimensionality of 200–500 is recommended as a “golden standard” \n",
    "2. **Random Projections, RP** aim to reduce vector space dimensionality. This is a very efficient (both memory- and CPU-friendly) approach to approximating tfidf distances between documents, by throwing in a little randomness. Recommended target dimensionality is again in the hundreds/thousands, depending on your dataset.\n",
    "3. **Latent Dirichlet Allocation, LDA** is yet another transformation from bag-of-words counts into a topic space of lower dimensionality. LDA is a probabilistic extension of LSA (also called multinomial PCA), so LDA's topics can be interpreted as probability distributions over words. These distributions are, just like with LSA, inferred automatically from a training corpus. Documents are in turn interpreted as a (soft) mixture of these topics (again, just like with LSA).\n",
    "4. **Hierarchical Dirichlet Process, HDP** is a non-parametric bayesian method (note the missing number of requested topics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(actual_dir)\n",
    "from similarity_matrix import get_similarity_matrix\n",
    "from matsim2np import matsim2np\n",
    "\n",
    "max_posts=len(df_token)\n",
    "num_best=max_posts+1\n",
    "article_ids=df_token.index\n",
    "num_sims=len(df_token) #Save only top ten similar articles\n",
    "\n",
    "# TFIDF\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "tfidf_corpus=tfidf[corpus]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LSI (Latent semantic indexing)\n",
    "run_lsi=True\n",
    "if (run_lsi):\n",
    "    num_topics=200 #dimensionality of model\n",
    "    topic_model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=num_topics)\n",
    "    similarity_matrix,matsim=get_similarity_matrix(article_ids,topic_model[tfidf_corpus],num_best,num_sims)\n",
    "    npmatrix=matsim2np(matsim)\n",
    "    npmatrix.dump('sim_matrix_lsi.pickle')\n",
    "\n",
    "# RP (Random projections)\n",
    "run_rp=True\n",
    "if ( run_rp ):\n",
    "    num_topics=200\n",
    "    topic_model=models.RpModel(tfidf_corpus,id2word=dictionary,num_topics=num_topics)\n",
    "    similarity_matrix_rp,matsim=get_similarity_matrix(article_ids,topic_model[tfidf_corpus],num_best,num_sims)\n",
    "    npmatrix=matsim2np(matsim)\n",
    "    npmatrix.dump('sim_matrix_rp.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rangel/anaconda3/envs/cdips2017/lib/python3.6/site-packages/gensim/models/ldamodel.py:529: RuntimeWarning: overflow encountered in exp2\n",
      "  (perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words))\n"
     ]
    }
   ],
   "source": [
    "# from similarity_matrix import get_similarity_matrix_sparse\n",
    "\n",
    "# LDA (Latent Dirichlet Allocation)\n",
    "run_lda=True\n",
    "if run_lda:\n",
    "    num_topics=200\n",
    "#    topic_model=models.LdaModel(corpus,id2word=dictionary,num_topics=num_topics)\n",
    "    topic_model=models.LdaModel(tfidf_corpus,id2word=dictionary,num_topics=num_topics)\n",
    "    similarity_matrix_lda,matsim=get_similarity_matrix(article_ids,topic_model[corpus],num_best,num_sims)\n",
    "    npmatrix=matsim2np(matsim)\n",
    "    npmatrix.dump('sim_matrix_lda.pickle')\n",
    "\n",
    "# HDP (Hierarchical Dirichlet Process)\n",
    "run_hdp=True\n",
    "if run_hdp:\n",
    "    num_topics=200\n",
    "#    topic_model=models.HdpModel(corpus,id2word=dictionary)\n",
    "    topic_model=models.HdpModel(tfidf_corpus,id2word=dictionary)\n",
    "    similarity_matrix_hdp,matsim=get_similarity_matrix(article_ids,topic_model[corpus],num_best,num_sims)\n",
    "    npmatrix=matsim2np(matsim)\n",
    "    npmatrix.dump('sim_matrix_hdp.pickle')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "os.chdir('../data')\n",
    "\n",
    "# Save similarity into files:\n",
    "# LSI\n",
    "df_similarity=pd.DataFrame.from_dict(similarity_matrix_lsi, orient='index')\n",
    "df_similarity.columns=['similar_articles','scores']\n",
    "df_similarity.to_csv('Wikipedia-sims-lsi.csv')\n",
    "\n",
    "# RP\n",
    "df_similarity=pd.DataFrame.from_dict(similarity_matrix_rp, orient='index')\n",
    "df_similarity.columns=['similar_articles','scores']\n",
    "df_similarity.to_csv('Wikipedia-sims-rp.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "# LDA\n",
    "df_similarity=pd.DataFrame.from_dict(similarity_matrix_lda, orient='index')\n",
    "df_similarity.columns=['similar_articles','scores']\n",
    "df_similarity.to_csv('Wikipedia-sims-lda.csv')\n",
    "\n",
    "# HDP\n",
    "df_similarity=pd.DataFrame.from_dict(similarity_matrix_lda, orient='index')\n",
    "df_similarity.columns=['similar_articles','scores']\n",
    "df_similarity.to_csv('Wikipedia-sims-hdp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
