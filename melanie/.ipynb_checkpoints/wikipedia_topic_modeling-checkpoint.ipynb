{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script makes parse_XML_with_minidom_ML more modular\n",
    "\n",
    "Also add url, page_type of articles\n",
    "\n",
    "Compare different models: LSI, RP, LDA, HDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melanie\\Anaconda3\\envs\\cdips2017\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from xml.dom import minidom\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('data')\n",
    "xmlfile = 'Wikipedia-20170717213140.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_page_type(title):\n",
    "    \"\"\"\n",
    "    from WikiPage.py: extract page type from article title\n",
    "    \"\"\"\n",
    "    if 'Category:' in title:\n",
    "        return 'category'\n",
    "    elif 'Portal:' in title:\n",
    "        return 'portal'\n",
    "    elif 'List of' in title:\n",
    "        return 'list'\n",
    "    elif 'File:' in title:\n",
    "        return 'file'\n",
    "    else:\n",
    "        return 'article'\n",
    "\n",
    "def xml_to_df(xmlfile):\n",
    "    \"\"\"\n",
    "    input: xml filename\n",
    "    output: data frame with columns: article id, title, url, page_type, tokenized text\n",
    "    \n",
    "    filter out pages that are not articles\n",
    "    \"\"\"\n",
    "    xmldoc = minidom.parse(xmlfile)\n",
    "    idlist = xmldoc.getElementsByTagName('id')\n",
    "    titlelist = xmldoc.getElementsByTagName('title')\n",
    "    textlist = xmldoc.getElementsByTagName('text')\n",
    "    \n",
    "    titles = [title.childNodes[0].data for title in titlelist]\n",
    "    urllist = ['https://en.wikipedia.org/wiki/%s' % (title.replace(' ', '_'))\n",
    "              for title in titles]\n",
    "    typelist = [get_page_type(title) for title in titles]\n",
    "    \n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    word_data = [(id.childNodes[0].data, title, url, page_type,\n",
    "                  tokenizer.tokenize(text.childNodes[0].data.lower()))\n",
    "                for id, title, url, page_type, text in zip(idlist, titles, urllist, typelist, textlist)\n",
    "                if page_type == 'article']\n",
    "    word_data_df = pd.DataFrame(word_data, columns=['id', 'title', 'url', 'type', 'words'])\n",
    "    #word_data_df.to_csv('word_data_df.csv')\n",
    "    return(word_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>type</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5957048</td>\n",
       "      <td>Kennel</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kennel</td>\n",
       "      <td>article</td>\n",
       "      <td>[about, shelter, for, dogs, and, cats, for, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>729436</td>\n",
       "      <td>Cynology</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cynology</td>\n",
       "      <td>article</td>\n",
       "      <td>[cynology, ipac, en, s, ᵻ, ˈ, n, ɒ, l, ə, dʒ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1764821</td>\n",
       "      <td>Pack (canine)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Pack_(canine)</td>\n",
       "      <td>article</td>\n",
       "      <td>[other, uses, wolfpack, disambiguation, image,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>547372987</td>\n",
       "      <td>Rare breed (dog)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Rare_breed_(dog)</td>\n",
       "      <td>article</td>\n",
       "      <td>[for, a, list, of, rare, dog, breeds, category...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>547375119</td>\n",
       "      <td>Dogs in ancient China</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dogs_in_ancient_...</td>\n",
       "      <td>article</td>\n",
       "      <td>[refimprove, date, december, 2008, originalres...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                  title  \\\n",
       "0    5957048                 Kennel   \n",
       "1     729436               Cynology   \n",
       "2    1764821          Pack (canine)   \n",
       "3  547372987       Rare breed (dog)   \n",
       "4  547375119  Dogs in ancient China   \n",
       "\n",
       "                                                 url     type  \\\n",
       "0               https://en.wikipedia.org/wiki/Kennel  article   \n",
       "1             https://en.wikipedia.org/wiki/Cynology  article   \n",
       "2        https://en.wikipedia.org/wiki/Pack_(canine)  article   \n",
       "3     https://en.wikipedia.org/wiki/Rare_breed_(dog)  article   \n",
       "4  https://en.wikipedia.org/wiki/Dogs_in_ancient_...  article   \n",
       "\n",
       "                                               words  \n",
       "0  [about, shelter, for, dogs, and, cats, for, th...  \n",
       "1  [cynology, ipac, en, s, ᵻ, ˈ, n, ɒ, l, ə, dʒ, ...  \n",
       "2  [other, uses, wolfpack, disambiguation, image,...  \n",
       "3  [for, a, list, of, rare, dog, breeds, category...  \n",
       "4  [refimprove, date, december, 2008, originalres...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_data_df = xml_to_df(xmlfile)\n",
    "word_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>type</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>787653468</td>\n",
       "      <td>Cat</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cat</td>\n",
       "      <td>article</td>\n",
       "      <td>[about, the, cat, species, that, is, commonly,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>41493138</td>\n",
       "      <td>Cat bite</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Cat_bite</td>\n",
       "      <td>article</td>\n",
       "      <td>[infobox, disease, name, cat, bite, image, cat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>701109732</td>\n",
       "      <td>Ancylostoma tubaeforme</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Ancylostoma_tuba...</td>\n",
       "      <td>article</td>\n",
       "      <td>[italic, title, taxobox, image, name, ancylost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>754619</td>\n",
       "      <td>Feline diseases</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Feline_diseases</td>\n",
       "      <td>article</td>\n",
       "      <td>[further, feline, zoonosis, see, cat, health, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>849619</td>\n",
       "      <td>International Cat Day</td>\n",
       "      <td>https://en.wikipedia.org/wiki/International_Ca...</td>\n",
       "      <td>article</td>\n",
       "      <td>[infobox, holiday, holiday_name, international...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                   title  \\\n",
       "89  787653468                     Cat   \n",
       "90   41493138                Cat bite   \n",
       "91  701109732  Ancylostoma tubaeforme   \n",
       "92     754619         Feline diseases   \n",
       "93     849619   International Cat Day   \n",
       "\n",
       "                                                  url     type  \\\n",
       "89                  https://en.wikipedia.org/wiki/Cat  article   \n",
       "90             https://en.wikipedia.org/wiki/Cat_bite  article   \n",
       "91  https://en.wikipedia.org/wiki/Ancylostoma_tuba...  article   \n",
       "92      https://en.wikipedia.org/wiki/Feline_diseases  article   \n",
       "93  https://en.wikipedia.org/wiki/International_Ca...  article   \n",
       "\n",
       "                                                words  \n",
       "89  [about, the, cat, species, that, is, commonly,...  \n",
       "90  [infobox, disease, name, cat, bite, image, cat...  \n",
       "91  [italic, title, taxobox, image, name, ancylost...  \n",
       "92  [further, feline, zoonosis, see, cat, health, ...  \n",
       "93  [infobox, holiday, holiday_name, international...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_data_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dictionary(documents):\n",
    "    \"\"\"\n",
    "    construct a dictionary, i.e. mapping btwn word ids and their freq of occurence in the whole corpus\n",
    "    filter dictionary to remove stopwords and words occuring < min_count times\n",
    "    \n",
    "    input: documents is an iterable consisting of all the words in the corpus \n",
    "    output: filtered dictionary\n",
    "    \"\"\"\n",
    "    dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "    stop_words = nltk.corpus.stopwords.words('english') \n",
    "    min_count = 2\n",
    "    stop_ids = [dictionary.token2id[word] for word in stop_words\n",
    "               if word in dictionary.token2id]\n",
    "    rare_ids = [id for id, freq in dictionary.dfs.items()\n",
    "                if freq < min_count]\n",
    "    dictionary.filter_tokens(stop_ids + rare_ids)\n",
    "    dictionary.compactify()\n",
    "    return(dictionary)\n",
    "\n",
    "def make_lsi_similarity_matrix(word_data_df):\n",
    "    \"\"\"\n",
    "    construct LSI (latent semantic indexing) model on Tfidf-transformed corpus, print model topics, \n",
    "    return similarity matrix.\n",
    "    \"\"\"\n",
    "    documents = word_data_df['words'].values\n",
    "    dictionary = make_dictionary(documents)\n",
    "    # convert corpus to vectors using bag-of-words representation, i.e. tuples of word indices and word counts\n",
    "    corpus = [dictionary.doc2bow(words) for words in documents]\n",
    "    # serialize corpus to disk (market matrix format)\n",
    "    corpora.mmcorpus.MmCorpus.serialize('wiki.mm', corpus) # we should build model, then serialize the model\n",
    "    # transform input data with TFIDF\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    # construct model\n",
    "    lsi = models.lsimodel.LsiModel(corpus=tfidf[corpus], id2word=dictionary, num_topics=50) \n",
    "    lsi.save('wiki.lsi')\n",
    "    for i, topic in enumerate(lsi.print_topics(5)[:3]):\n",
    "        print('Topic: ', format(i))\n",
    "        print(str(topic).replace(' + ', '\\n')) \n",
    "        print('') \n",
    "    # use gensim to create similarity matrix\n",
    "    matsim = similarities.MatrixSimilarity(lsi[tfidf[corpus]], num_best=6)\n",
    "    return(matsim)\n",
    "\n",
    "def print_similar_articles(word_data_df, matsim, num_print):\n",
    "    \"\"\"\n",
    "    print titles of first num_print articles and their most similar articles and similarity scores.\n",
    "    this is independent of model used.\n",
    "    \"\"\"\n",
    "    titles = word_data_df['title']\n",
    "    # for the first num_print articles, print most similar articles and their similarity scores\n",
    "    for sims in list(matsim)[:num_print]:\n",
    "        title_index = sims[0][0]\n",
    "        print(titles[title_index]) \n",
    "        for other_title_index, score in sims[1:]:\n",
    "            print('\\t', titles[other_title_index], ' ', score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:  0\n",
      "(0, '0.301*\"cat\"\n",
      "0.219*\"cats\"\n",
      "0.192*\"dog\"\n",
      "0.155*\"journal\"\n",
      "0.115*\"meat\"\n",
      "0.094*\"name\"\n",
      "0.091*\"dogs\"\n",
      "0.090*\"feral\"\n",
      "0.090*\"doi\"\n",
      "0.089*\"volume\"')\n",
      "\n",
      "Topic:  1\n",
      "(1, '0.415*\"cat\"\n",
      "0.345*\"cafe\"\n",
      "-0.316*\"dog\"\n",
      "0.250*\"cats\"\n",
      "0.138*\"café\"\n",
      "-0.132*\"dogs\"\n",
      "-0.130*\"breed\"\n",
      "-0.111*\"wagging\"\n",
      "-0.111*\"kennel\"\n",
      "-0.104*\"breeds\"')\n",
      "\n",
      "Topic:  2\n",
      "(2, '0.273*\"meat\"\n",
      "-0.253*\"bites\"\n",
      "0.224*\"cafe\"\n",
      "-0.222*\"bite\"\n",
      "-0.173*\"rabies\"\n",
      "-0.161*\"cdc\"\n",
      "0.144*\"festival\"\n",
      "0.135*\"china\"\n",
      "-0.127*\"infection\"\n",
      "-0.114*\"wagging\"')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lsi_matsim = make_lsi_similarity_matrix(word_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kennel\n",
      "\t Cattery   0.814780771732\n",
      "\t Indian National Kennel Club   0.573465406895\n",
      "\t Dog World (newspaper)   0.334808826447\n",
      "\t Breed type (dog)   0.3020645082\n",
      "\t Lists of dogs   0.271350085735\n",
      "Cynology\n",
      "\t Felinology   0.48828369379\n",
      "\t Cat training   0.420496493578\n",
      "\t Pussy   0.377251684666\n",
      "\t Breed type (dog)   0.296519637108\n",
      "\t Pack (canine)   0.261724859476\n",
      "Pack (canine)\n",
      "\t Canid hybrid   0.631985366344\n",
      "\t Origin of the domestic dog   0.611404001713\n",
      "\t Dog   0.514093697071\n",
      "\t Canine reproduction   0.488296687603\n",
      "\t Cat training   0.443291783333\n",
      "Rare breed (dog)\n",
      "\t Breed type (dog)   0.882066190243\n",
      "\t Lists of dogs   0.285880744457\n",
      "\t Dog   0.284888446331\n",
      "\t Dog bite   0.282038062811\n",
      "\t Cynology   0.226576387882\n",
      "Dogs in ancient China\n",
      "\t Dogs in Mesoamerica   0.659915149212\n",
      "\t Dogs in religion   0.606493592262\n",
      "\t Panhu   0.505573272705\n",
      "\t Origin of the domestic dog   0.472601473331\n",
      "\t Dog   0.389480471611\n",
      "Dog biscuit\n",
      "\t Dog food   0.84871327877\n",
      "\t Dog daycare   0.35606315732\n",
      "\t Cat meat   0.250419855118\n",
      "\t Dog meat   0.249963223934\n",
      "\t Cat anatomy   0.228101164103\n",
      "Breed type (dog)\n",
      "\t Rare breed (dog)   0.882066190243\n",
      "\t Lists of dogs   0.529282033443\n",
      "\t Dog World (newspaper)   0.467361748219\n",
      "\t Indian National Kennel Club   0.402414470911\n",
      "\t Kennel   0.3020645082\n",
      "Canid hybrid\n",
      "\t Interbreeding of dingoes with other domestic dogs   0.74710804224\n",
      "\t Origin of the domestic dog   0.724725723267\n",
      "\t Pack (canine)   0.631985366344\n",
      "\t Dog   0.629116654396\n",
      "\t Canine reproduction   0.355261206627\n",
      "Canine physical therapy\n",
      "\t Therapy cat   0.797551453114\n",
      "\t Human–canine bond   0.467561215162\n",
      "\t Cat anatomy   0.371932446957\n",
      "\t Cynophobia   0.354084283113\n",
      "\t Dog   0.122435458004\n",
      "Dogs in Mesoamerica\n",
      "\t Dogs in ancient China   0.659915149212\n",
      "\t Cats in ancient Egypt   0.466625064611\n",
      "\t Dogs in religion   0.455452412367\n",
      "\t Origin of the domestic dog   0.351288735867\n",
      "\t Dog food   0.335316568613\n"
     ]
    }
   ],
   "source": [
    "print_similar_articles(word_data_df, lsi_matsim, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_rp_similarity_matrix(word_data_df):\n",
    "    \"\"\"\n",
    "    construct RP (random projections) model on Tfidf-transformed corpus, print model topics, \n",
    "    return similarity matrix.\n",
    "    \"\"\"\n",
    "    documents = word_data_df['words'].values\n",
    "    dictionary = make_dictionary(documents)\n",
    "    # convert corpus to vectors using bag-of-words representation, i.e. tuples of word indices and word counts\n",
    "    corpus = [dictionary.doc2bow(words) for words in documents]\n",
    "    # serialize corpus to disk (market matrix format)\n",
    "    corpora.mmcorpus.MmCorpus.serialize('wiki.mm', corpus) # we should build model, then serialize the model\n",
    "    # transform input data with TFIDF\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    # construct model\n",
    "    rp = models.Rpmodel(corpus=tfidf[corpus], num_topics=10)\n",
    "    rp.save('wiki.rp_model')\n",
    "    #lsi = models.lsimodel.LsiModel(corpus=tfidf[corpus], id2word=dictionary, num_topics=10) \n",
    "    #lsi.save('wiki.lsi')\n",
    "    for i, topic in enumerate(rp.print_topics(5)[:3]):\n",
    "        print('Topic: ', format(i))\n",
    "        print(str(topic).replace(' + ', '\\n')) \n",
    "        print('') \n",
    "    # use gensim to create similarity matrix\n",
    "    matsim = similarities.MatrixSimilarity(rp[tfidf[corpus]], num_best=6)\n",
    "    return(matsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module gensim.models.rpmodel in gensim.models:\n",
      "\n",
      "NAME\n",
      "    gensim.models.rpmodel\n",
      "\n",
      "DESCRIPTION\n",
      "    # -*- coding: utf-8 -*-\n",
      "    #\n",
      "    # Copyright (C) 2010 Radim Rehurek <radimrehurek@seznam.cz>\n",
      "    # Licensed under the GNU LGPL v2.1 - http://www.gnu.org/licenses/lgpl.html\n",
      "\n",
      "CLASSES\n",
      "    gensim.interfaces.TransformationABC(gensim.utils.SaveLoad)\n",
      "        RpModel\n",
      "    \n",
      "    class RpModel(gensim.interfaces.TransformationABC)\n",
      "     |  Objects of this class allow building and maintaining a model for Random Projections\n",
      "     |  (also known as Random Indexing). For theoretical background on RP, see:\n",
      "     |  \n",
      "     |    Kanerva et al.: \"Random indexing of text samples for Latent Semantic Analysis.\"\n",
      "     |  \n",
      "     |  The main methods are:\n",
      "     |  \n",
      "     |  1. constructor, which creates the random projection matrix\n",
      "     |  2. the [] method, which transforms a simple count representation into the TfIdf\n",
      "     |     space.\n",
      "     |  \n",
      "     |  >>> rp = RpModel(corpus)\n",
      "     |  >>> print(rp[some_doc])\n",
      "     |  >>> rp.save('/tmp/foo.rp_model')\n",
      "     |  \n",
      "     |  Model persistency is achieved via its load/save methods.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RpModel\n",
      "     |      gensim.interfaces.TransformationABC\n",
      "     |      gensim.utils.SaveLoad\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, bow)\n",
      "     |      Return RP representation of the input vector and/or corpus.\n",
      "     |  \n",
      "     |  __init__(self, corpus, id2word=None, num_topics=300)\n",
      "     |      `id2word` is a mapping from word ids (integers) to words (strings). It is\n",
      "     |      used to determine the vocabulary size, as well as for debugging and topic\n",
      "     |      printing. If not set, it will be determined from the corpus.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  initialize(self, corpus)\n",
      "     |      Initialize the random projection matrix.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  save(self, fname_or_handle, separately=None, sep_limit=10485760, ignore=frozenset(), pickle_protocol=2)\n",
      "     |      Save the object to file (also see `load`).\n",
      "     |      \n",
      "     |      `fname_or_handle` is either a string specifying the file name to\n",
      "     |      save to, or an open file-like object which can be written to. If\n",
      "     |      the object is a file handle, no special array handling will be\n",
      "     |      performed; all attributes will be saved to the same file.\n",
      "     |      \n",
      "     |      If `separately` is None, automatically detect large\n",
      "     |      numpy/scipy.sparse arrays in the object being stored, and store\n",
      "     |      them into separate files. This avoids pickle memory errors and\n",
      "     |      allows mmap'ing large arrays back on load efficiently.\n",
      "     |      \n",
      "     |      You can also set `separately` manually, in which case it must be\n",
      "     |      a list of attribute names to be stored in separate files. The\n",
      "     |      automatic check is not performed in this case.\n",
      "     |      \n",
      "     |      `ignore` is a set of attribute names to *not* serialize (file\n",
      "     |      handles, caches etc). On subsequent load() these attributes will\n",
      "     |      be set to None.\n",
      "     |      \n",
      "     |      `pickle_protocol` defaults to 2 so the pickled object can be imported\n",
      "     |      in both Python 2 and 3.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  load(fname, mmap=None) from builtins.type\n",
      "     |      Load a previously saved object from file (also see `save`).\n",
      "     |      \n",
      "     |      If the object was saved with large arrays stored separately, you can load\n",
      "     |      these arrays via mmap (shared memory) using `mmap='r'`. Default: don't use\n",
      "     |      mmap, load large arrays as normal objects.\n",
      "     |      \n",
      "     |      If the file being loaded is compressed (either '.gz' or '.bz2'), then\n",
      "     |      `mmap=None` must be set.  Load will raise an `IOError` if this condition\n",
      "     |      is encountered.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    logger = <Logger gensim.models.rpmodel (WARNING)>\n",
      "\n",
      "FILE\n",
      "    c:\\users\\melanie\\anaconda3\\envs\\cdips2017\\lib\\site-packages\\gensim\\models\\rpmodel.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(models.rpmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gensim.models.rpmodel' has no attribute 'Rpmodel'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-f0964ba0e4d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrp_matsim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_rp_similarity_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_data_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-7cd0ece6862a>\u001b[0m in \u001b[0;36mmake_rp_similarity_matrix\u001b[1;34m(word_data_df)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTfidfModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# construct model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mrp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrpmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRpmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mrp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wiki.rp_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m#lsi = models.lsimodel.LsiModel(corpus=tfidf[corpus], id2word=dictionary, num_topics=10)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'gensim.models.rpmodel' has no attribute 'Rpmodel'"
     ]
    }
   ],
   "source": [
    "rp_matsim = make_rp_similarity_matrix(word_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot projection of articles onto 2 axes/topics defined by the model; print 10 most impt words above plot\n",
    "def format_topic_coeffs(topic):\n",
    "    \"\"\"Return a list of coefficent, word tuples with coefficent truncated to \n",
    "    3 decimal places.\n",
    "    \"\"\"\n",
    "    return [(format(coeff), word) for coeff, word in topic]\n",
    "\n",
    "def plot_axes(x=0, y=1, model=lsi, corpus=corpus, \n",
    "              tfidf=tfidf, titles=titles):\n",
    "    \"\"\"Plot each article title according to the projection of its text \n",
    "    into the given x and y topic axes of model.\n",
    "    \n",
    "    :param x: the index of the x axis to plot\n",
    "    :param y: the index of the y axis to plot\n",
    "    :param model: the gensim model to project into\n",
    "    :param corpus: the gensim corpus of documents\n",
    "    :param tfidf: a tfidf model for converting documents into tfidf space\n",
    "    :param titles: a list of article titles\n",
    "    \"\"\"\n",
    "    x_data = defaultdict(list) \n",
    "    y_data = defaultdict(list) \n",
    "    arts = defaultdict(list)  \n",
    "    print('x topic:')\n",
    "    pprint(format_topic_coeffs(model.show_topic(x)))\n",
    "    print('')\n",
    "    print('y topic:') \n",
    "    pprint(format_topic_coeffs(model.show_topic(y)))\n",
    "    for title, doc in zip(titles, corpus):\n",
    "        x_data[0].append((model[tfidf[doc]][x][1]))\n",
    "        y_data[0].append((model[tfidf[doc]][y][1]))\n",
    "        arts[0].append(title)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    ax = plt.gca()\n",
    "    plt.scatter(x_data[0], y_data[0], s=40)\n",
    "    for art, x, y in zip(arts[0], x_data[0], y_data[0]):\n",
    "        ax.annotate(str(art), xy=(x, y), xycoords='data', xytext=(1, 1), \n",
    "        textcoords='offset points', size=10)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_axes(x=0, y=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_axes(x=2, y=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_axes(x=4, y=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
